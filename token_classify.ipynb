{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "token_classify.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seeger22/jzou2/blob/master/token_classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAm7_9cUf41f"
      },
      "source": [
        "# Token classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns4CAn8NTf2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27954cd6-de56-4299-c562-ab386396de52"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9S5YxIA5ux1",
        "outputId": "2d0b3a2b-e8ed-459a-e0ee-bb0ea9b9bf85"
      },
      "source": [
        "%env HOME=/content/drive/MyDrive/\n",
        "!mkdir -p ~/Research/huggingface\n",
        "%cd ~/Research/huggingface"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: HOME=/content/drive/MyDrive/\n",
            "/content/drive/MyDrive/Research/huggingface\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZbh5EYzmWlU"
      },
      "source": [
        "## Install huggingface stuff\r\n",
        "Huggingface's transformers library, pretrained models, and datasets have very good support.\r\n",
        "\r\n",
        "* `transformers` library\r\n",
        "* `datasets` library. Also look at [Colab notebook](https://github.com/huggingface/datasets/blob/master/notebooks/Overview.ipynb) for details.\r\n",
        "\r\n",
        "Besides, we need to install other library, including sequential evaluation metrics\r\n",
        "* `seqeval` library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfHHqkSEgO6Y"
      },
      "source": [
        "# You are supposed to run just after you first run this notebook! We want to fix this library to ensure reproducibility.\r\n",
        "!git clone https://github.com/huggingface/transformers.git\r\n",
        "!git clone https://github.com/huggingface/datasets.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgRFKZheovHL"
      },
      "source": [
        "# You are supposed to run everytime you run this Colab notebook\r\n",
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k98D97qdp3oT"
      },
      "source": [
        "# You are supposed to run everytime you run this Colab notebook\r\n",
        "#%cd ~/Research/huggingface/transformers/\r\n",
        "#!python setup.py install\r\n",
        "\r\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVIpV5OurOKp"
      },
      "source": [
        "# You are supposed to run everytime you run this Colab notebook\r\n",
        "#%cd ~/Research/huggingface/datasets/\r\n",
        "#!python setup.py install\r\n",
        "\r\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bqc8M2sr7MD"
      },
      "source": [
        "# You are supposed to run everytime you run this Colab notebook\r\n",
        "\r\n",
        "# Make sure that we have a recent version of pyarrow in the session before we continue - otherwise reboot Colab to activate it\r\n",
        "import pyarrow\r\n",
        "if int(pyarrow.__version__.split('.')[1]) < 16 and int(pyarrow.__version__.split('.')[0]) == 0:\r\n",
        "    import os\r\n",
        "    os.kill(os.getpid(), 9)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPuyYkYdnAsc"
      },
      "source": [
        "# sanity check\r\n",
        "from transformers import BertForTokenClassification\r\n",
        "from datasets import ClassLabel, load_dataset"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyOB0tDogsQo"
      },
      "source": [
        "## Run torch example\r\n",
        "* See transformers/examples, there are many other task examples to copy from.\\\r\n",
        "* Refer to trasnformers/examples/README.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgx5GsE_iefk"
      },
      "source": [
        "%cd ~/Research/huggingface/transformers/examples/token-classification\r\n",
        "!bash run.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8E0xmr8jDVc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNCT0KvGwkxR"
      },
      "source": [
        "# Task 1: Run on DSTC9 data\r\n",
        "* Goal: Extract named entities such as hotel names, restaurant names that are defined in a provided knowledge base file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6znZRt6pA6t"
      },
      "source": [
        "!git clone https://github.com/alexa/alexa-with-dstc9-track1-dataset.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keUJLNDknhJD",
        "outputId": "0ebae0f6-d34d-45dd-f441-6b5460a5cd0a"
      },
      "source": [
        "%cd ~/Research/ner/"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Research/ner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBIQk9mcl0Rq",
        "outputId": "c5920234-3404-4d14-a8e1-d76d5d6d29cd"
      },
      "source": [
        "%%writefile BabyTrie.py\r\n",
        "import re\r\n",
        "\r\n",
        "class BabyTrie:#baby version of Trie\r\n",
        "    class TrieNode:#Node within a Trie\r\n",
        "        def __init__(self,word=None):\r\n",
        "            self.children={}#dictionary of TrieNodes\r\n",
        "            self.markers=[]#list of markers/categories\r\n",
        "            self.end=False#the Node is leaf, or end of word\r\n",
        "            self.word=word#in case needed\r\n",
        "    \r\n",
        "    def __init__(self):\r\n",
        "        self.root=self.new_node()\r\n",
        "\r\n",
        "    def new_node(self,word=None):\r\n",
        "        #Creates TrieNode object with a given word\r\n",
        "        return self.TrieNode(word)\r\n",
        "\r\n",
        "    def insert(self,lst,cat):\r\n",
        "        '''\r\n",
        "        Given a list of words that make up a Named Entity and its category,\r\n",
        "        inserts the Entity into the Trie.\r\n",
        "\r\n",
        "        Note: the structure resembles a tree, i.e. words as nodes, and at the end\r\n",
        "        of the inserted word, the node is made a leaf node (end=True), and a Marker\r\n",
        "        is added to the node's list of markers.\r\n",
        "        -------------------------------------------------------------------\r\n",
        "        Example:\r\n",
        "        bt=BabyTrie()\r\n",
        "        possible_NE=['Jade','Garden']#let's say that this is a restaurant\r\n",
        "        bt.insert(possible_NE,'restaurant')\r\n",
        "        '''\r\n",
        "        if cat not in lst:\r\n",
        "            lst.append(cat)\r\n",
        "        '''\r\n",
        "        for the sake of more possible matches:\r\n",
        "        If inserted \"Jade Garden\" and it's a restaurant,\r\n",
        "        \"Jade Garden\" will be recognized as an entity;\r\n",
        "        and \"Jade Garden Restaurant\" will also be one.\r\n",
        "        '''\r\n",
        "        ptr=self.root\r\n",
        "        for i in range(0,len(lst)-1):#for every elem but not the last (since it is the category)\r\n",
        "            if lst[i] not in ptr.children.keys():#if already a key, skip; else add new\r\n",
        "                new=self.new_node(lst[i])\r\n",
        "                ptr.children[lst[i]]=new\r\n",
        "            ptr=ptr.children[lst[i]]\r\n",
        "        ptr.end=True#is end of word\r\n",
        "        if cat not in ptr.markers:#Only adds new categories to the list of markers \r\n",
        "            ptr.markers.append(cat)\r\n",
        "        if lst[-1] not in ptr.children.keys():#avoid possible conflict with same name\r\n",
        "            new=self.new_node(lst[-1])\r\n",
        "            ptr.children[lst[-1]]=new#adds the category as one of the leaf node\r\n",
        "        ptr=ptr.children[lst[-1]]\r\n",
        "        ptr.end=True\r\n",
        "        if cat not in ptr.markers:#only adds category if not in list of markers\r\n",
        "            ptr.markers.append(cat)\r\n",
        "    \r\n",
        "    def isinTrie(self,sen):\r\n",
        "        '''\r\n",
        "        Given a sentence/string,\r\n",
        "        returns a tuple ((1),(2)) where:\r\n",
        "                 (1) is a list of the stripped, lowercased, and split (even punctuations)\r\n",
        "        sentence, mainly for the convenience of tokenization.\r\n",
        "                 (2) is a dictionary with keys as categories and their values as tuples where\r\n",
        "        the first index is the starting position and second index is the ending position\r\n",
        "        of the Named Entities in the sentence.\r\n",
        "\r\n",
        "        Note: these Named Entities are defined on the BabyTrie.\r\n",
        "        Note also:\r\n",
        "        1. While doing the clean-up for the sentence, if given string is \"McDonald's\" or anything\r\n",
        "        with RE structure ^(\\w+)(\\'w+)$ will be split as (\\w+) and (\\'w+), i.e. \"McDonald\" and \"'s\"\r\n",
        "        2. While cleaning up the dictionary for output, the longest instance is always taken.\r\n",
        "        A node can also have multiple markers, in the case where both instances happen and have\r\n",
        "        the same exact distances, both will be counted towards the final result. For further\r\n",
        "        information please see the comment for each condition and the final output.\r\n",
        "        -------------------------------------------------------------------------------------\r\n",
        "        Example:\r\n",
        "        #bt is already defined in the above example, which has an inserted entity\r\n",
        "        test_sentence='I went to Jade Garden last night.'\r\n",
        "        res=bt.isinTrie(test_sentence)\r\n",
        "        print(res[0],'\\n',res[1])\r\n",
        "\r\n",
        "        Output:\r\n",
        "        ['i','went','to','jade','garden','last','night','.']\r\n",
        "        {'restaurant':(3,4)}\r\n",
        "        '''\r\n",
        "        dic={}#the uncleaned version of returned dictionary\r\n",
        "        flag=False#=True when a phrase in the sentence does not match the trie anymore\r\n",
        "        lst=[]#the list that will ultimately be returned\r\n",
        "        new_sen=sen.lower().strip()#cleans up sentence\r\n",
        "        plst=re.findall(r\"[\\w']+|[.,!?;]\", new_sen)#premature list used for clean-up\r\n",
        "        p=re.compile(r\"^(\\w+)(\\'\\w+)$\")\r\n",
        "        for i in range (len(plst)):\r\n",
        "            if p.match(plst[i]):\r\n",
        "                new_word1,new_word2=p.match(plst[i]).group(1),p.match(plst[i]).group(2)\r\n",
        "                lst.append(new_word1)\r\n",
        "                lst.append(new_word2)\r\n",
        "            else:\r\n",
        "                lst.append(plst[i])\r\n",
        "        \r\n",
        "        #the returned list is completely formed at this point, we use it to generate dictionary\r\n",
        "        rstart=0#starting position of NE\r\n",
        "        rend=0#ending position of NE\r\n",
        "        last=len(lst)-1#keep track of last to ensure index does not go over limit\r\n",
        "\r\n",
        "        for i in range (len(lst)):\r\n",
        "            ptr=self.root\r\n",
        "            if lst[i] in ptr.children:\r\n",
        "                rstart=i\r\n",
        "                ptr=ptr.children[lst[i]]\r\n",
        "                for j in range (i+1,len(lst)):\r\n",
        "                    if lst[j] not in ptr.children:\r\n",
        "                        flag=True\r\n",
        "                        rend=j-1\r\n",
        "                        break\r\n",
        "                    ptr=ptr.children[lst[j]]\r\n",
        "                    rend=j\r\n",
        "                \r\n",
        "                if ptr.end:\r\n",
        "                    for i in range(len(ptr.markers)):\r\n",
        "                        if (ptr.markers[i] in dic):#if already a key, add to value\r\n",
        "                            dic[ptr.markers[i]].append((rstart,rend))\r\n",
        "                        else:#if not, make new entry\r\n",
        "                            dic[ptr.markers[i]]=[(rstart,rend)]\r\n",
        "                    rstart=0\r\n",
        "                    rend=0\r\n",
        "\r\n",
        "        #cleaning up dictionary\r\n",
        "        rdic={}#clean version of the dictionary that is ultimately returned\r\n",
        "        for cat in dic.keys():\r\n",
        "            rdic[cat]=[]\r\n",
        "            rlst=sorted(dic[cat],key=lambda item:item[1]-item[0],reverse=True)\r\n",
        "            rdic[cat].append(rlst[0])\r\n",
        "            for i in range(1,len(rlst)):\r\n",
        "                rflag=True\r\n",
        "                counter=1\r\n",
        "                for rcat in rdic.keys():#make sure no overlap: ex. 'some hotel diner' > 'some hotel'\r\n",
        "                    for elem in rdic[rcat]:\r\n",
        "                        if (rlst[i][0]<elem[0] and rlst[i][1]<elem[0]):#ex. if we have (3,5) we can take (1,2)\r\n",
        "                            continue\r\n",
        "                        elif (rlst[i][0]>elem[1] and rlst[i][1]>elem[1]):#ex. if we have (3,5) we can take (6,7)\r\n",
        "                            continue\r\n",
        "                        else:# any equals, [0] or [1] will not work: ex. we have (3,5). cannot have (3,4) or (4,5).\r\n",
        "                            rflag=False\r\n",
        "                if rflag:\r\n",
        "                    rdic[cat].append(rlst[i])\r\n",
        "        return (lst,rdic)\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting BabyTrie.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9udWeITzmC6V",
        "outputId": "3aecf262-ced4-4db0-a147-0cb35cdc8e43"
      },
      "source": [
        "%%writefile preprocess.py\r\n",
        "import json\r\n",
        "import re\r\n",
        "import argparse\r\n",
        "from BabyTrie import BabyTrie\r\n",
        "from argparse import Namespace\r\n",
        "\r\n",
        "def get_bt(dic):\r\n",
        "    '''\r\n",
        "    Given a dictionary with categories of entities in the form:\r\n",
        "    {'some category':{'0':{'name':'some name'},'1':{'name':'some other name'},...},'some other category':{'0':...},...}\r\n",
        "    Return a BabyTrie that contains all entities and the leaf nodes contianing a list of their corresponding categories\r\n",
        "    \r\n",
        "    Note: A large portion of this code under the if statement is cleaning up the list of words within the name phrase\r\n",
        "    -------------------------------------------------------------------------------------------------------------------\r\n",
        "    Example:\r\n",
        "    dic={'restaurant':{'0':{'name':'Jade Garden'}}}\r\n",
        "    bt=get_bt(dic)#the name 'jade' and 'garden' are created as nodes and inserted into the BabyTrie\r\n",
        "    \r\n",
        "    '''\r\n",
        "    bt=BabyTrie()\r\n",
        "    for cat in dic.keys():\r\n",
        "        for elem in dic[cat]:\r\n",
        "            record = dic[cat][elem]\r\n",
        "            if ('name' in record and record['name'] is not None):\r\n",
        "                new_name=record['name'].lower().strip()#cleans up name\r\n",
        "                lst=[]#final list that is used to insert into bt\r\n",
        "                plst=re.findall(r\"[\\w']+|[.,!?;]\", new_name)#premature list, can have ex. \"McDonald's\" instead of \"McDonald\" and \"'s\"\r\n",
        "                p=re.compile(r\"^(\\w+)(\\'\\w+)$\")\r\n",
        "                for i in range (len(plst)):\r\n",
        "                    if p.match(plst[i]):\r\n",
        "                        new_word1,new_word2=p.match(plst[i]).group(1),p.match(plst[i]).group(2)\r\n",
        "                        lst.append(new_word1)\r\n",
        "                        lst.append(new_word2)\r\n",
        "                    else:\r\n",
        "                        lst.append(plst[i])\r\n",
        "            bt.insert(lst,cat)\r\n",
        "    return bt\r\n",
        "\r\n",
        "def extraction_of_log1(dic):#will generate dupes. Eliminate after everything\r\n",
        "    lst=[]\r\n",
        "    for i in range(0,len(dic)):\r\n",
        "        for elem in dic[i]:\r\n",
        "            lst.append(elem[\"text\"])\r\n",
        "    return lst\r\n",
        "\r\n",
        "def extraction_of_log2(dic):\r\n",
        "    lst=[]\r\n",
        "    for cat in dic.keys():\r\n",
        "        for elem in dic[cat]:\r\n",
        "            for dialogue_num in dic[cat][elem]['docs']:\r\n",
        "                lst.append(dic[cat][elem]['docs'][dialogue_num]['title'])\r\n",
        "                lst.append(dic[cat][elem]['docs'][dialogue_num]['body'])\r\n",
        "    return lst\r\n",
        "\r\n",
        "def printres(ofstream,res):\r\n",
        "    '''\r\n",
        "    Given a result tuple that is generated from the isinTrie method in the class of BabyTrie,\r\n",
        "    Prints the result in two columns containing labels.\r\n",
        "    Note: the labeling of 'B_cat','I_cat', and 'O' is used.\r\n",
        "    B_cat = begining of an entity of a category\r\n",
        "    I_cat = intermediate (in the middle) of an entity of a category\r\n",
        "    O = not an entity\r\n",
        "    ----------------------------------------------------------------------------------------\r\n",
        "    Example: (same output from isinTrie example)\r\n",
        "    res=(['i','went','to','jade','garden','last','night','.'], {'restaurant':(3,4)})\r\n",
        "                        lst                                            dic\r\n",
        "    printres(res)\r\n",
        "\r\n",
        "    Output:\r\n",
        "    i              O\r\n",
        "    went           O\r\n",
        "    to             O\r\n",
        "    jade           B_restaurant\r\n",
        "    garden         I_restaurant\r\n",
        "    last           O\r\n",
        "    night          O\r\n",
        "    .              O\r\n",
        "    '''\r\n",
        "    lst=res[0]\r\n",
        "    dic=res[1]\r\n",
        "    sen_label=[[word,'O'] for word in lst]\r\n",
        "    for cat in dic.keys():\r\n",
        "        for item in dic[cat]:\r\n",
        "            start,end=item[0],item[1]\r\n",
        "            first=True\r\n",
        "            for i in range(start,end+1):\r\n",
        "                if (sen_label[i][-1]!='O'):\r\n",
        "                    if first:\r\n",
        "                        record=' | B_'+cat\r\n",
        "                        first=False\r\n",
        "                    else:\r\n",
        "                        record=' | I_'+cat\r\n",
        "                    sen_label[i][-1]+=record\r\n",
        "                else:\r\n",
        "                    if first:\r\n",
        "                        sen_label[i][-1]='B_'+cat\r\n",
        "                        first=False\r\n",
        "                    else:\r\n",
        "                        sen_label[i][-1]='I_'+cat\r\n",
        "    for elem in sen_label:\r\n",
        "        ofstream.write('{}\\t{}\\n'.format(elem[0],elem[1]))\r\n",
        "    ofstream.write('\\n')\r\n",
        "def main():\r\n",
        "    parser=argparse.ArgumentParser(description='NER preprocess dataset')\r\n",
        "    parser.add_argument(\"--log_file\",type=str,default='logs.json',help=\"input log file in json\")\r\n",
        "    parser.add_argument(\"--knowledge_file\",type=str,default='knowledge.json',help=\"input knowledge file in json\")\r\n",
        "    parser.add_argument(\"--output_file\",type=str,default='output.txt',help=\"output file for NER training\")\r\n",
        "    parser.add_argument(\"--extract_lognum\",type=int,default=2,help=\"1 for log, 2 for log included in knowledge base\")\r\n",
        "    args = parser.parse_args()\r\n",
        "    \r\n",
        "    #opens files first to load\r\n",
        "    logs1=open(args.log_file,'r')\r\n",
        "    data1=json.load(logs1)\r\n",
        "    k=open(args.knowledge_file,'r')\r\n",
        "    dic=json.load(k)\r\n",
        "    \r\n",
        "\r\n",
        "    bt=get_bt(dic)#setting up bt based on knowledge base\r\n",
        "    if args.extract_lognum==1:\r\n",
        "        dset=extraction_of_log1(data1)#extract logs1\r\n",
        "    elif args.extract_lognum==2:\r\n",
        "        dset=extraction_of_log2(dic)#extract additional data from knowledge base\r\n",
        "    \r\n",
        "    outFile=open(args.output_file,'w')\r\n",
        "    for sen in dset:#write results of log to file\r\n",
        "        res=bt.isinTrie(sen)\r\n",
        "        printres(outFile,res)\r\n",
        "    \r\n",
        "    logs1.close()\r\n",
        "    k.close()\r\n",
        "    outFile.close()\r\n",
        "    \r\n",
        "main()\r\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting preprocess.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "dePhV-qsOPvr",
        "outputId": "0dc2a191-2d3c-4589-cc7f-0a948bf93ad0"
      },
      "source": [
        "!git init\r\n",
        "!git config --global user.email\"seeger22@126.com\"\r\n",
        "!git config --global user.name\"seeger22\"\r\n",
        "!git add -A\r\n",
        "!git commit -m \"first commit\"\r\n",
        "\r\n",
        "'''\r\n",
        "!mkdir ./temp\r\n",
        "!git clone https://github.com/seeger22/ner.git ./temp\r\n",
        "!rsync -aP --exclude=data/ \"drive/MyDrive/Research/ner\"/* ./temp\r\n",
        "\r\n",
        "%cd ./temp\r\n",
        "!git add .\r\n",
        "!git commit -m '\"yuh\"'\r\n",
        "!git config --global user.email \"seeger22@126.com\"\r\n",
        "!git config --global user.name \"seeger22\"\r\n",
        "!git push origin \"master\"\r\n",
        "%cd /content\r\n",
        "!rm -rf ./temp\r\n",
        "'''"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reinitialized existing Git repository in /content/.git/\n",
            "^C\n",
            "On branch master\n",
            "\n",
            "Initial commit\n",
            "\n",
            "Untracked files:\n",
            "\t\u001b[31m.config/\u001b[m\n",
            "\t\u001b[31mBabyTrie.py\u001b[m\n",
            "\t\u001b[31mdrive/\u001b[m\n",
            "\t\u001b[31mpreprocess.py\u001b[m\n",
            "\t\u001b[31msample_data/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!mkdir ./temp\\n!git clone https://github.com/seeger22/ner.git ./temp\\n!rsync -aP --exclude=data/ \"drive/MyDrive/Research/ner\"/* ./temp\\n\\n%cd ./temp\\n!git add .\\n!git commit -m \\'\"yuh\"\\'\\n!git config --global user.email \"seeger22@126.com\"\\n!git config --global user.name \"seeger22\"\\n!git push origin \"master\"\\n%cd /content\\n!rm -rf ./temp\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_O61jzMwBkO"
      },
      "source": [
        "# Task 2: Incorporate dictionary features into the model\r\n",
        "\r\n",
        "* Use Trie to locate the positions of each entity in a sentence.\r\n",
        "* Feed in positions as extra features into the model.\r\n",
        "* During test time, we can add new entities from knowledge base into the dictionary to improve model robustness on recognizing the new named entities not covered during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztjjKzitw6TI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}